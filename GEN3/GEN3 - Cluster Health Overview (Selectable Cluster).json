{"version":14,"variables":[{"key":"Cluster","type":"query","input":"fetch dt.entity.kubernetes_cluster\n| fields entity.name\n| sort entity.name asc","multiple":false,"defaultValue":"postgres-dev-c01-mkp-nonp-lx-f03"}],"tiles":{"0":{"type":"data","title":"Node CPU Utilization","query":"// Fetch the cpu usage and cpu allocatable timeseries metrics and split by both node and cluster. We will calculate percentage using these two metrics.\ntimeseries {\n  cpu_usage = sum(dt.kubernetes.container.cpu_usage, rollup:avg),\n  cpu_allocatable = sum(dt.kubernetes.node.cpu_allocatable, rollup:avg)\n}, by:{ dt.entity.kubernetes_cluster, dt.entity.kubernetes_node }\n\n// Use the internal ID of the node to get the real name of the node.  Same for the cluster.\n| join on: { left[dt.entity.kubernetes_node]==right[id] }, [fetch dt.entity.kubernetes_node], fields: {nodeName = entity.name}\n| join on: { left[dt.entity.kubernetes_cluster]==right[id] }, [fetch dt.entity.kubernetes_cluster], fields: {clusterName = entity.name}\n\n// Filter down to just the cluster we care about\n| filter clusterName == $Cluster\n\n// Calculate CPU percentage\n| fieldsAdd nodeName, cpu_usage_percent = (cpu_usage[] / cpu_allocatable[]) * 100\n\n// Remove the fields we do not want to display\n| fieldsRemove dt.entity.kubernetes_node, dt.entity.kubernetes_cluster, cpu_usage, cpu_allocatable, clusterName","davis":{"enabled":false,"davisVisualization":{"isAvailable":true}},"visualization":"lineChart","visualizationSettings":{"thresholds":[],"chartSettings":{"gapPolicy":"connect","circleChartSettings":{"groupingThresholdType":"relative","groupingThresholdValue":0,"valueType":"relative"},"categoryOverrides":{},"categoricalBarChartSettings":{"categoryAxis":"nodeName","categoryAxisLabel":"nodeName","valueAxis":"interval","valueAxisLabel":"interval"},"hiddenLegendFields":[],"fieldMapping":{"timestamp":"timeframe","leftAxisValues":["cpu_usage_percent"],"leftAxisDimensions":["nodeName"]}},"singleValue":{"showLabel":true,"label":"","prefixIcon":"","recordField":"dt.entity.kubernetes_cluster","autoscale":true,"alignment":"center","colorThresholdTarget":"value"},"table":{"rowDensity":"condensed","enableSparklines":false,"hiddenColumns":[],"lineWrapIds":[],"columnWidths":{}},"histogram":{"dataMappings":[{"valueAxis":"interval","rangeAxis":""}]},"honeycomb":{"shape":"hexagon","dataMappings":{"category":"nodeName","value":"nodeName"}},"unitsOverrides":[{"identifier":"cpu_usage_percent","unitCategory":"percentage","baseUnit":"percent","displayUnit":null,"decimals":0,"suffix":"","delimiter":false,"added":1715220070059}]}},"1":{"type":"data","title":"Node Memory Utilization","query":"// This query displays the memory utilization, by node, for the specified cluster.  Simply change the cluster filter to the name of the cluster.\n\n\n// Fetch the memory usage and memory allocatable timeseries metrics and split by both node and cluster. We will calculate percentage using these two metrics.\ntimeseries {\n  memory_usage = sum(dt.kubernetes.container.memory_working_set, rollup:avg),\n  memory_allocatable = sum(dt.kubernetes.node.memory_allocatable, rollup:avg)\n}, by:{ dt.entity.kubernetes_cluster, dt.entity.kubernetes_node }\n\n// Use the internal ID of the node to get the real name of the node.  Same for the cluster.\n| join on: { left[dt.entity.kubernetes_node]==right[id] }, [fetch dt.entity.kubernetes_node], fields: {nodeName = entity.name}\n| join on: { left[dt.entity.kubernetes_cluster]==right[id] }, [fetch dt.entity.kubernetes_cluster], fields: {clusterName = entity.name}\n\n// Filter down to just the cluster we care about\n| filter clusterName == $Cluster\n\n// Calculate memory percentage\n| fieldsAdd nodeName, memory_usage_percent = (memory_usage[] / memory_allocatable[]) * 100\n\n// Remove the fields we do not want to display\n| fieldsRemove dt.entity.kubernetes_node, dt.entity.kubernetes_cluster, memory_usage, memory_allocatable, clusterName","davis":{"enabled":false,"davisVisualization":{"isAvailable":true}},"visualization":"lineChart","visualizationSettings":{"thresholds":[],"chartSettings":{"gapPolicy":"connect","circleChartSettings":{"groupingThresholdType":"relative","groupingThresholdValue":0,"valueType":"relative"},"categoryOverrides":{},"categoricalBarChartSettings":{"categoryAxis":"nodeName","categoryAxisLabel":"nodeName","valueAxis":"interval","valueAxisLabel":"interval"},"hiddenLegendFields":[],"fieldMapping":{"timestamp":"timeframe","leftAxisValues":["memory_usage_percent"],"leftAxisDimensions":["nodeName"]}},"singleValue":{"showLabel":true,"label":"","prefixIcon":"","recordField":"nodeName","autoscale":true,"alignment":"center","colorThresholdTarget":"value"},"table":{"rowDensity":"condensed","enableSparklines":false,"hiddenColumns":[],"lineWrapIds":[],"columnWidths":{}},"histogram":{"dataMappings":[{"valueAxis":"interval","rangeAxis":""}]},"honeycomb":{"shape":"hexagon","dataMappings":{"category":"nodeName","value":"nodeName"}},"unitsOverrides":[{"identifier":"memory_usage_percent","unitCategory":"percentage","baseUnit":"percent","displayUnit":null,"decimals":0,"suffix":"","delimiter":false,"added":1715220238692}]}},"2":{"type":"data","title":"Count of \"Unhealthy\" Events by Node","query":"// Fetch all events\nfetch events\n\n// Filter down to just unhealthy events.  \n| filter dt.kubernetes.event.reason == \"Unhealthy\"\n\n// These events are at the pod level, but we'll report on the node and cluster containing the pod\n| summarize Count = count(), by: { dt.entity.kubernetes_node, dt.entity.kubernetes_cluster }\n\n// Get the real name of the node and cluster\n| join on: { left[dt.entity.kubernetes_node]==right[id] }, [fetch dt.entity.kubernetes_node], fields: {Node = entity.name}\n| join on: { left[dt.entity.kubernetes_cluster]==right[id] }, [fetch dt.entity.kubernetes_cluster], fields: {Cluster = entity.name}\n\n| filter Cluster == $Cluster\n// Specify the order of the fields and sort by cluster then node\n| fields Node, Count\n| sort Node","davis":{"enabled":false,"davisVisualization":{"isAvailable":true}},"visualization":"pieChart","visualizationSettings":{"thresholds":[],"chartSettings":{"gapPolicy":"connect","circleChartSettings":{"groupingThresholdType":"relative","groupingThresholdValue":0,"valueType":"relative"},"categoryOverrides":{},"categoricalBarChartSettings":{"categoryAxis":"Node","categoryAxisLabel":"Node","valueAxis":"Count","valueAxisLabel":"Count"}},"singleValue":{"showLabel":true,"label":"","prefixIcon":"","recordField":"Cluster","autoscale":true,"alignment":"center","colorThresholdTarget":"value"},"table":{"rowDensity":"condensed","enableSparklines":false,"hiddenColumns":[],"lineWrapIds":[],"columnWidths":{}},"histogram":{"dataMappings":[{"valueAxis":"Count","rangeAxis":""}]},"honeycomb":{"shape":"hexagon","dataMappings":{"category":"Node","value":"Count"}},"unitsOverrides":[{"identifier":"Count","unitCategory":"unspecified","baseUnit":"none","displayUnit":null,"decimals":0,"suffix":"events","delimiter":false,"added":1715284551289}]}},"3":{"type":"data","title":"Listing of all \"Unhealthy\" Events for Timeframe","query":"// Fetch all events\nfetch events\n\n// Filter down to just unhealthy events.  \n| filter dt.kubernetes.event.reason == \"Unhealthy\"\n\n// Get the real name of the node and cluster\n| join on: { left[dt.entity.kubernetes_node]==right[id] }, [fetch dt.entity.kubernetes_node], fields: {Node = entity.name}\n| join on: { left[dt.entity.kubernetes_cluster]==right[id] }, [fetch dt.entity.kubernetes_cluster], fields: {Cluster = entity.name}\n\n| filter Cluster == $Cluster\n\n// Select the fields we're going to display in the table and sort\n| fields timestamp, Pod = dt.kubernetes.event.involved_object.name, Name = event.name, Node\n// | fields timestamp, TargetObjectType = dt.kubernetes.event.involved_object.kind, TargetObject = dt.kubernetes.event.involved_object.name, Reason = dt.kubernetes.event.reason, Name = event.name, Node \n| sort timestamp desc","davis":{"enabled":false,"davisVisualization":{"isAvailable":true}},"visualization":"table","visualizationSettings":{"thresholds":[],"chartSettings":{"gapPolicy":"connect","circleChartSettings":{"groupingThresholdType":"relative","groupingThresholdValue":0,"valueType":"relative"},"categoryOverrides":{},"categoricalBarChartSettings":{}},"singleValue":{"showLabel":true,"label":"","prefixIcon":"","recordField":"timestamp","autoscale":true,"alignment":"center","colorThresholdTarget":"value"},"table":{"rowDensity":"condensed","enableSparklines":false,"hiddenColumns":[],"lineWrapIds":[],"columnWidths":{"[\"TargetObject\"]":152.125}},"histogram":{"dataMappings":[]},"honeycomb":{"shape":"hexagon","dataMappings":{"category":"Pod","value":"timestamp"}}}},"4":{"type":"data","title":"All Pending Pods","query":"fetch dt.entity.cloud_application_instance\n| filter cloudApplicationInstancePhase == \"PENDING\"\n| fieldsFlatten clustered_by\n| join on: { left[clustered_by.dt.entity.kubernetes_cluster]==right[id] }, [fetch dt.entity.kubernetes_cluster], fields: {dt.entity.kubernetes_cluster.name = entity.name}\n| filter dt.entity.kubernetes_cluster.name == $Cluster\n| fields Pod = entity.name, Workload = workloadName, Node = nodeName, Namespace = namespaceName\n| sort Pod","davis":{"enabled":false,"davisVisualization":{"isAvailable":true}},"visualization":"table","visualizationSettings":{"thresholds":[],"chartSettings":{"gapPolicy":"connect","circleChartSettings":{"groupingThresholdType":"relative","groupingThresholdValue":0,"valueType":"relative"},"categoryOverrides":{},"categoricalBarChartSettings":{"categoryAxisLabel":"Pod","valueAxisLabel":"CPULimits"}},"singleValue":{"showLabel":true,"label":"","prefixIcon":"","recordField":"Pod","autoscale":true,"alignment":"center","colorThresholdTarget":"value"},"table":{"rowDensity":"condensed","enableSparklines":false,"hiddenColumns":[],"lineWrapIds":[],"columnWidths":{"[\"Pod\"]":222.765625}},"histogram":{"dataMappings":[]},"honeycomb":{"shape":"hexagon","dataMappings":{"category":"Pod","value":"Workload"}}}},"5":{"type":"data","title":"All Pods Without CPU/Memory Requests or Limits","query":"// Fetch all pods\nfetch dt.entity.cloud_application_instance\n\n// Filter down to only those that meet at least one of the offending conditions that we're looking for\n| filter isNull (limitsCPU) OR isNull (limitsMemory) OR isNull (requestsCPU) or isNull (requestsMemory)\n\n// Flatten the \"clustered by\" field since it's a complex record. This extracts the internal ID of the cluster into a new field we use in next step.\n| fieldsFlatten clustered_by\n\n// Fetch the real name of the cluster from the associated cluster entity\n| join on: { left[clustered_by.dt.entity.kubernetes_cluster]==right[id] }, [fetch dt.entity.kubernetes_cluster], fields: {dt.entity.kubernetes_cluster.name = entity.name}\n\n| filter dt.entity.kubernetes_cluster.name == $Cluster\n// Assign friendly names to all fields and specify the order of the columns\n| fields Pod = entity.name, Workload = workloadName, Node = nodeName, Namespace = namespaceName, CPULimits = limitsCPU, CPURequests = requestsCPU, MemoryLimits = limitsMemory, MemoryRequests = requestsMemory\n\n// Sort by pod name\n| sort Pod","davis":{"enabled":false,"davisVisualization":{"isAvailable":true}},"visualization":"table","visualizationSettings":{"thresholds":[],"chartSettings":{"gapPolicy":"connect","circleChartSettings":{"groupingThresholdType":"relative","groupingThresholdValue":0,"valueType":"relative"},"categoryOverrides":{},"categoricalBarChartSettings":{"categoryAxis":"Pod","categoryAxisLabel":"Pod","valueAxis":"CPULimits","valueAxisLabel":"CPULimits"}},"singleValue":{"showLabel":true,"label":"","prefixIcon":"","recordField":"Pod","autoscale":true,"alignment":"center","colorThresholdTarget":"value"},"table":{"rowDensity":"condensed","enableSparklines":false,"hiddenColumns":[],"lineWrapIds":[],"columnWidths":{"[\"Pod\"]":163.07812500000003,"[\"Workload\"]":216.046875,"[\"Node\"]":187.375,"[\"MemoryLimits\"]":106.734375}},"histogram":{"dataMappings":[{"valueAxis":"CPULimits","rangeAxis":""},{"valueAxis":"CPURequests","rangeAxis":""},{"valueAxis":"MemoryLimits","rangeAxis":""},{"valueAxis":"MemoryRequests","rangeAxis":""}]},"honeycomb":{"shape":"hexagon","dataMappings":{"category":"Pod","value":"Workload"}}}},"6":{"type":"markdown","title":"","content":"*Refer to the associated [Dynatrace Notebook](https://flu19434.apps.dynatrace.com/ui/apps/dynatrace.notebooks/notebook/f796b2c8-b87c-4488-95a5-78d0d681fdb1) for details on how the tiles on this dashboard function and alternatives for viewing or how to be alerted to problems.*"},"7":{"type":"markdown","title":"","content":"# Pod Analysis"},"8":{"type":"markdown","title":"","content":"# Resource Utilization and Cluster Availability\n*Dynatrace automatically creates problems related to resource utilization and alerts the appropriate team*"},"9":{"type":"data","title":"Cluster Availability","query":"timeseries {\n    pinged = avg( dt.kubernetes.cluster.readyz)\n}, by: { dt.entity.kubernetes_cluster }\n\n| join on: { left[dt.entity.kubernetes_cluster]==right[id] }, [fetch dt.entity.kubernetes_cluster], fields: {clusterName = entity.name}\n\n| filter clusterName == $Cluster\n\n| fieldsAdd availability = pinged [] * 100\n\n| fieldsRemove dt.entity.kubernetes_cluster, pinged\n","davis":{"enabled":false,"davisVisualization":{"isAvailable":true}},"visualization":"barChart","visualizationSettings":{"thresholds":[],"chartSettings":{"gapPolicy":"connect","circleChartSettings":{"groupingThresholdType":"relative","groupingThresholdValue":0,"valueType":"relative"},"categoryOverrides":{},"categoricalBarChartSettings":{"categoryAxis":"clusterName","categoryAxisLabel":"clusterName","valueAxis":"interval","valueAxisLabel":"interval"},"hiddenLegendFields":[],"fieldMapping":{"timestamp":"timeframe","leftAxisValues":["availability"],"leftAxisDimensions":["clusterName"]},"legend":{"hidden":true},"valueRepresentation":"relative"},"singleValue":{"showLabel":true,"label":"","prefixIcon":"","recordField":"clusterName","autoscale":true,"alignment":"center","colorThresholdTarget":"value"},"table":{"rowDensity":"condensed","enableSparklines":false,"hiddenColumns":[],"lineWrapIds":[],"columnWidths":{}},"histogram":{"dataMappings":[{"valueAxis":"interval","rangeAxis":""}]},"honeycomb":{"shape":"hexagon","dataMappings":{"category":"clusterName","value":"clusterName"}},"unitsOverrides":[{"identifier":"availability","unitCategory":"percentage","baseUnit":"percent","displayUnit":null,"decimals":0,"suffix":"","delimiter":false,"added":1715288740232}]}},"10":{"type":"markdown","title":"","content":"*Dynatrace automatically alerts you to node resource issues*"},"11":{"type":"markdown","title":"","content":"*Dynatrace automatically alerts you to cluster availability issues*"},"12":{"type":"data","title":"Pending System Pods","query":"fetch dt.entity.cloud_application_instance\n| filter cloudApplicationInstancePhase == \"PENDING\"\n| fieldsFlatten clustered_by\n| join on: { left[clustered_by.dt.entity.kubernetes_cluster]==right[id] }, [fetch dt.entity.kubernetes_cluster], fields: {dt.entity.kubernetes_cluster.name = entity.name}\n| filter dt.entity.kubernetes_cluster.name == $Cluster\n| fields Pod = entity.name, Workload = workloadName, Node = nodeName, Namespace = namespaceName\n| filter contains(Namespace, \"system\")\n| sort Pod","davis":{"enabled":false,"davisVisualization":{"isAvailable":true}},"visualization":"table","visualizationSettings":{"thresholds":[],"chartSettings":{"gapPolicy":"connect","circleChartSettings":{"groupingThresholdType":"relative","groupingThresholdValue":0,"valueType":"relative"},"categoryOverrides":{},"categoricalBarChartSettings":{"categoryAxisLabel":"Pod","valueAxisLabel":"CPULimits"}},"singleValue":{"showLabel":true,"label":"","prefixIcon":"","recordField":"Pod","autoscale":true,"alignment":"center","colorThresholdTarget":"value"},"table":{"rowDensity":"condensed","enableSparklines":false,"hiddenColumns":[],"lineWrapIds":[],"columnWidths":{}},"histogram":{"dataMappings":[]},"honeycomb":{"shape":"hexagon","dataMappings":{"category":"Pod","value":"Workload"}}}},"13":{"type":"markdown","title":"","content":"# Postgres Database Metrics"},"14":{"type":"data","title":"Persistent Volume Claim Utilization","query":"timeseries {\n    used = avg ( dt.kubernetes.persistentvolumeclaim.used ),\n\tcapacity = avg ( dt.kubernetes.persistentvolumeclaim.capacity )\n}, by: { k8s.persistent_volume_claim.name, k8s.cluster.name}\n\n| filter k8s.cluster.name == $Cluster\n| fieldsAdd utilization = (used[] / capacity[]) * 100\n\n| fieldsRemove used, capacity, k8s.cluster.name","davis":{"enabled":false,"davisVisualization":{"isAvailable":true}},"visualization":"lineChart","visualizationSettings":{"thresholds":[],"chartSettings":{"gapPolicy":"connect","circleChartSettings":{"groupingThresholdType":"relative","groupingThresholdValue":0,"valueType":"relative"},"categoryOverrides":{},"hiddenLegendFields":[],"fieldMapping":{"timestamp":"timeframe","leftAxisValues":["utilization"],"leftAxisDimensions":["k8s.persistent_volume_claim.name"]},"categoricalBarChartSettings":{"categoryAxisLabel":"k8s.persistent_volume_claim.name","valueAxisLabel":"interval","categoryAxis":"k8s.persistent_volume_claim.name","valueAxis":"interval"}},"singleValue":{"showLabel":true,"label":"","prefixIcon":"","autoscale":true,"alignment":"center","colorThresholdTarget":"value","recordField":"tablespace"},"table":{"rowDensity":"condensed","enableSparklines":false,"hiddenColumns":[],"lineWrapIds":[],"columnWidths":{}},"histogram":{"dataMappings":[{"valueAxis":"interval","rangeAxis":""}]},"honeycomb":{"shape":"hexagon","dataMappings":{"category":"k8s.persistent_volume_claim.name","value":"k8s.persistent_volume_claim.name"}},"unitsOverrides":[{"identifier":"utilization","unitCategory":"percentage","baseUnit":"percent","displayUnit":null,"decimals":0,"suffix":"","delimiter":false,"added":1715291371073}]}},"15":{"type":"markdown","title":"","content":"# API Server Availability"},"16":{"type":"markdown","title":"","content":"*Dynatrace could monitor this several ways, and they are not mutually exclusive:*\n* *Ingest the Log Insight alert*\n* *Monitor whatever Log Insight is monitoring (a log file) and put anomaly detection around it*\n* *Synthetic HTTP testing directly against the API server*\n* *Full-stack monitoring of the entire application stack to know immediately when services calling the API server begin to encounter issues*"}},"layouts":{"0":{"x":0,"y":3,"w":10,"h":3},"1":{"x":10,"y":3,"w":10,"h":3},"2":{"x":0,"y":10,"w":9,"h":5},"3":{"x":9,"y":10,"w":14,"h":5},"4":{"x":0,"y":15,"w":11,"h":4},"5":{"x":0,"y":19,"w":23,"h":6},"6":{"x":0,"y":0,"w":23,"h":1},"7":{"x":0,"y":9,"w":23,"h":1},"8":{"x":0,"y":1,"w":23,"h":2},"9":{"x":0,"y":6,"w":20,"h":3},"10":{"x":20,"y":3,"w":3,"h":3},"11":{"x":20,"y":6,"w":3,"h":3},"12":{"x":11,"y":15,"w":12,"h":4},"13":{"x":0,"y":25,"w":23,"h":1},"14":{"x":0,"y":26,"w":23,"h":6},"15":{"x":0,"y":32,"w":23,"h":1},"16":{"x":0,"y":33,"w":23,"h":3}}}